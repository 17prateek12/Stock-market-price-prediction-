# -*- coding: utf-8 -*-
"""TESLA2final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4lfPWlXu5hUj3vtEgmNz1bNH7vOpfvV
"""

import pandas_datareader as pdr
import pandas as pd
import numpy as np
from numpy import array
import tensorflow as tf
import math
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.metrics import mean_squared_error
from datetime import datetime as dt

key="f67c57eadb50e9ee8d3e84f8595cf9ff70922cbd"
df = pdr.get_data_tiingo('TSLA',start='2011-01-17', end=dt.now(), api_key=key)
df.to_csv('TSLA.csv')
df=pd.read_csv('TSLA.csv')

df.head()

df.tail()

df.head()

df1=df.reset_index()['adjClose']

plt.plot(df1)

#moving avg
ma100 = df.adjClose.rolling(100).mean()
ma200 = df.adjClose.rolling(200).mean()
ma100,ma200

plt.figure(figsize = (24 ,12))
plt.plot(df.adjClose)
plt.plot(ma100,'g', label ='ma 100')
plt.plot(ma200,'r',label ='ma 200')

scaler=MinMaxScaler(feature_range=(0,1))#data preprocessing LSTM are sensitive to scale of the data  #values b/w 0and 1
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
print(df1)

##splitting dataset into train and test split
training_size=int(len(df1)*0.75)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
training_size,test_size

# this is for time dependency
def create_dataset(dataset, time_step=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-time_step-1):
		a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
		dataX.append(a)
		dataY.append(dataset[i + time_step, 0])
	return np.array(dataX), np.array(dataY)

time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
print(X_train.shape), print(y_train.shape)

print(X_test.shape), print(ytest.shape)

# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1) #converting it into 3d
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)

model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(X_train.shape[1],1)))#activation function -  relu
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')#https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/

model.summary()

model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=350,batch_size=64,verbose=1)

### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)

model.save("lstm_model.h5")

print(train_predict)

print(test_predict)

y_testreal = ytest
y_testreal = y_testreal.reshape(-1, 1)
y_testreal=scaler.inverse_transform(y_testreal)
print(y_testreal)

math.sqrt(mean_squared_error(y_train,train_predict))

math.sqrt(mean_squared_error(ytest,test_predict))

look_back=100

trainPredictPlot = np.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = np.empty_like(df1)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.figure(figsize = (24 ,6))
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

len(train_data)

len(test_data)

x=len(test_data)-100
print(x)

x_input=test_data[x:].reshape(1,-1)
x_input.shape

temp_input=list(x_input)
temp_input=temp_input[0].tolist()
# temp_input

lst_output=[]
n_steps=100
i=0
while(i<20):   
  if(len(temp_input)>100):
    #print(temp_input)
    x_input=np.array(temp_input[1:])
    # print("{} day input {}".format(i,x_input))
    x_input=x_input.reshape(1,-1)
    x_input = x_input.reshape((1, n_steps, 1))
    #print(x_input)
    yhat = model.predict(x_input, verbose=0)
    y_hat = yhat
    y_hat = scaler.inverse_transform(y_hat)
    print("{} day output {}".format(i+1,y_hat))
    temp_input.extend(yhat[0].tolist())
    temp_input=temp_input[1:]
    #print(temp_input)
    lst_output.extend(yhat.tolist())
    i=i+1
  else:
    x_input = x_input.reshape((1, n_steps,1))
    yhat = model.predict(x_input, verbose=0)
    # print(yhat[0])
    temp_input.extend(yhat[0].tolist())
    # print(len(temp_input))
    lst_output.extend(yhat.tolist())
    i=i+1

day_new=np.arange(1,101)
day_pred=np.arange(101,121)

plt.plot(day_new,scaler.inverse_transform(df1[len(df1)-100:]),label='Trained data')
plt.plot(day_pred, scaler.inverse_transform(lst_output),label='Next 20 days')

#acc
x = math.sqrt(mean_squared_error(y_train,train_predict))
x

type(x)

a = 522.6069 + x
acc= (x/a)*100
acc = 100- acc
print('The accuracy is :')
print(acc)

"""The accuracy for AdjClose is : 80.14157881339638 for data of TESLA from 2018-01-17 to 2023-01-13

The accuracy for AdjClose is : 97.20555127257244 for data of TESLA from 2011-01-18 to 2023-01-13
"""